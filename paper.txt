Remote biometric advertisements research in pandemic time. Is this a viable alternative to laboratory-based studies?











1. Introduction 
This paper focuses on the biometric analysis of video advertisements for products and services of technological companies. The objective is to analyse how respondents perceive these advertisements and the emotions they are likely to experience while watching them. Subsequently, this was compared to the findings of a parallel survey conducted concurrently.
With the restrictions on in-person interactions and the need to maintain social distancing, traditional methods of biometric research that require physical contact or presence became impractical, potentially hazardous and in fact almost impossible.
Remote biometric research has emerged as a critical tool during the time of the pandemic, offering numerous advantages in terms of data collection, participant safety, and accessibility therefore, many of them at that time were carried out in fields such as health and medicine (Maurya et al., 2023), psychology and neuroscience (Healey & Picard, 2020), human-computer interaction (Krafka et al., 2020) or sports (Aroganam et al., 2019). iMotions released online remote collection feature in May 2020, reacted quickly for challenges posed by the COVID-19 pandemic and therefore this environment was chosen to conduct the remote experiment accessible for respondents via dedicated link using a desktop or laptop computer equipped with a webcam.
Four video advertisements from leading computer hardware and software providers were for the study:
•        Dell (https://youtu.be/d7FfGtg660o) - a professional IT consulting service for businesses and 24/7 support in this area.
•        Google (https://youtu.be/ymMmJITn2ZM) - showcasing how using Google services in everyday life can help oneself and others.
•        Microsoft (https://youtu.be/utanXLR0nO0) - a humorous comparison of the advantages of the Microsoft Surface Pro 7 computer with the competing Apple iPad Pro (satirical).
•        HP (https://youtu.be/A74R7sCxjrw) - a funny promotion featuring a vampire couple endorsing the HP printer ink subscription service, with automatic ordering of ink cartridges through the device in a door-to-door model, ensuring a continuous supply.
The study involved the presentation of video advertisements and the completion of an online survey. The cameras in respondents' computers recorded their facial expressions, which were synchronized in real-time on a timeline using the iMotions software. This allowed further analysis of facial expressions and eye movements in specific areas of interest during the analysis phase.
Functioning within the academic environment determined the choice of the research group, comprising mostly students and their acquaintances. However, going beyond the walls of the university, the survey link was also sent via Facebook and crossed-shared between people.
Regarding the segmentation of the research sample, the most interesting division appeared to be between individuals who were enthusiastic about technology and those who did not consider themselves technology enthusiasts. This choice was driven, on one hand, by the research topic itself and, on the other hand, by the desire to compare the differences in advertisement perception based on the respondents' familiarity with the broad IT industry. During the conceptual stage, factors such as gender and age (with the youngest respondent being 16 years old and the oldest being 66) held less significance. However, these factors were ultimately included in the survey as they may prove useful for further analysis and potential broader comparisons.
In the end, a total of 90 fully completed surveys were registered between November 15th and December 18th, 2021. These surveys involved respondents who completed the entire study process, and their responses were correctly recorded and exported to the iMotions cloud computing system. 5 research sessions were rejected for reasons such as: incompleteness (interruption of the remote examination), abnormally low sampling of facetracking or eyetracking resulting from the respondent's insufficient equipment or lack of processed data due to the face being invisible during the examination despite its visibility at the stage of checking the preliminary conditions by the system .
Analysis of facial expressions was conducted using the AFFECTIVA module, which provided 20 facial action units, seven basic emotions (joy, anger, fear, disgust, contempt, sadness, and surprise), facial characteristics, and behavioral indicators such as head orientation and attention. These measures provided values that represented the likelihood of expressing the expected emotion. This algorithm was a powerful research tool as emotions were processed based on a neural network developed from the analysis of approximately one billion video frames of recorded faces (McDuff & el Kaliouby, 2013).
Webcam-Based Eye tracking module was used to analyze how respondents viewed individual elements of the advertisements, focusing on areas of interest. This thesis employed a simplified algorithm for eye movement analysis based on a webcam. Under stationary conditions, a professional tool such as an eye tracker could provide more accurate and precise biometric data. However, due to the pandemic and the remote mode of study necessitated by it, such equipment was not feasible.
The built-in Survey engine within the iMotions software was used to collect survey data remotely from the respondents. The first survey collected basic respondent data and allowed for segmentation. The second survey not only covered the respondent's attitude towards the brand/product/service but also included requests to specify experienced and non-experienced emotions and to describe a distinctive element (or elements) of the advertisement that was memorable. Questions related to emotions are intended for further comparison of declared emotions in relation to those registered using the Affectiva algorithm. Additionally, the question about a distinctive element of the advertisement enables the confrontation of the provided responses with the video recorded through webcam-based eye-tracking.
Collected data was first stored in the iMotions software provider's cloud computing system and then imported and processed using the desktop software of the same company. Additional analysis was conducted using the bioinformatics programming language R in the R Studio environment.
The study combined declarative methods (survey) with non-declarative methods (hard data), specifically eye-tracking and facial emotion analysis. By employing both methods, an attempt was made to compare and triangulate the collected data. The analysis of the gathered research material enabled to describe the emotions evoked by specific segments of the selected advertisements and, additionally, identified the visual elements that attracted respondents' attention the most. The respondents' answers to the survey questions were also crucial for the entire experiment.
2. Research procedure and experimental setup
All The following tools were used to create the experiment:
•        iMotions Biometric Research Platform 9.0;
•        iMotions Facial Expression Analysis Module (Affectiva AFFDEX);
•        iMotions Web Eye tracking Module;
•        iMotions Survey Module;
•        iMotions Remote Data Collection Module.
Each respondent conducted the study on their own computer using a web browser, with access to the study provided through a previously sent link.


  

Figure 1. Online study - environment check.


The study consisted of the following elements:
1.        Displaying and collecting necessary consents (conscious participation in the study, anonymized processing of biometric and sensitive data - biometric tests conducted at the Laboratory of Media Studies have a positive opinion of the university ethics committee).
2.        The algorithm requested access to the webcam, checked head position and lighting, and allowed progression to the next step only after meeting the requirements.
3.        The respondent was presented a brief instruction regarding the study.
4.        Initial calibration of web-based eye tracking.
5.        Displaying segmentation survey.
6.        The respondent's baseline emotions were assessed using the Karolinska Directed Emotional Faces (KDEF) image set. Respondents viewed human faces depicting anger, contempt, disgust, happiness, sadness, and surprise, and were then asked to reflect these emotions on their own faces.
7.        The respondent watched a video ad and completed a survey regarding emotions, brand perception, and attention-grabbing elements (four ads and four polls, after a second ad, web-based eye tracking intermediate calibration was performed (required for more accurate measurements).
8.        Final calibration of the web-cam based eye tracking was performed (required for maximum accuracy).
9.        A thank-you message for participating in the study was displayed, and the study data was saved and sent to the iMotions cloud.
3. Limitations and encountered problems
Considering the remote nature of the study, the following issues and limitations had to be addressed:
•        Participants conducted the study using their own equipment, with different configurations and varying quality, which especially in the case of webcams can affect the quality of eye tracking measurements and the performance of the facial emotion recognition algorithm;
•        The reliability of the web-cam based eye tracking algorithm may be questionable due to its operating principles and the stage of its development;
•        Poor internet connection quality for some participants may prevent proper completion of the study;
•        Participants may not complete the study due to boredom or closure of the study before the complete upload of their data recording to the iMotions cloud platform.
The first fear was unfounded, the quality of the AFFECTIVA measurement for the entire study was very high - as much as 98% (24Hz), which - taking into account the fact that individual respondents joined the study remotely and used different equipment - is a phenomenal result. For comparison, the quality of the eye tracking measurement was assessed at 70% accuracy and 24Hz, which is also a good result considering the fact that we are dealing with webcam-based measurements.
The second concern is problematic to some extent. The method was chosen on purpose, firstly, to be able to conduct the study in the conditions of a pandemic, and secondly, to test its operation in practice. The camera-based method requires a fixed one and bright lighting, and to keep head movements to a minimum. To sum up, even by ensuring optimal lighting conditions and maximally limiting head movements, it must be clearly stated that the examination carried out using a dedicated eye-tracker is more accurate in every respect, which has been measured and proven by the creators of the iMotions software. Even under ideal conditions and with the very professional Logitech Brio camera, the ability to recognize fine spatial details shifts in dynamic objects during head fixation or in static objects during head or body rotation (dva) was significantly higher when measured with a webcam. At the time of writing this text, the algorithm was updated to the WebET 3.0 version and now the dva indices are more favourable for this method, they are 2.2, and 0.5 for the stationary eyetracker, for comparison, when the experiment was conducted in the WebET 2.0 version, dva was 4.3 for the webcam method and 0.6 for the stationary eye tracker, respectively. This shows that the method is constantly being improved and has good prospects.


  

Figure 2. DVA for webcam based vs. stationary eyetracker (by iMotions).


Webcam-based eye tracking and stationary eye tracking are two different methods used to track eye movements, each with its own advantages and limitations. Webcam-based eye tracking utilizes the built-in camera of a computer or a smartphone to capture and analyze the user's eye movements. This method relies on computer vision algorithms to track the position of the eyes and estimate gaze direction (Sugano et al., 2014; Recasens et al., 2015; Wood et al., 2016). It offers the advantage of being easily accessible and cost-effective since it eliminates the need for specialized hardware. However, webcam-based eye tracking may be less accurate and reliable compared to stationary eye trackers. The resolution and frame rate of the camera, as well as lighting conditions, can affect the accuracy of eye tracking measurements. Additionally, the webcam's position and angle can limit the field of view, potentially missing some eye movements. On the other hand, stationary eye trackers are dedicated hardware devices specifically designed for eye tracking research (Klaib et al., 2021). These devices typically use infrared light and specialized sensors to accurately track the position of the eyes and measure eye movements. Stationary eye trackers offer high precision and accuracy, allowing for detailed analysis of eye gaze patterns and fixation durations. They are often used in scientific studies and professional applications that require precise eye tracking measurements. However, stationary eye trackers can be more expensive and less portable compared to webcam-based eye tracking solutions. In summary, webcam-based eye tracking is a convenient and accessible option that can be used with existing devices, but it may have limitations in terms of accuracy and reliability. Stationary eye trackers, while more expensive and less portable, provide highly accurate and detailed eye tracking measurements, making them suitable for research and professional applications that demand precise data. The choice between these two methods depends on the specific requirements of the eye tracking task and the available resources. As mentioned before, in the case of this study, the measurement quality of web-based eye tracking was assessed at 70%, with a sampling rate of 24Hz. For comparison, using a stationary eye tracker, the current standard is sampling rates around 120-250Hz, and when conducting similar studies in laboratory conditions, we encounter measurement quality within the range of 90-98 percent. When conducting an experiment using iMotions in a web-cam based eye tracking setup, three calibrations are required: initial, intermediate, and final. In comparison, when conducting studies with stationary eye tracker, only one initial calibration is necessary, which significantly shortens the entire research procedure. It is important to consider a significant advantage of the stationary eye tracker, both in terms of the quantity of gathered data and the significantly lower amount of lost data.
The third and fourth concerns seem to be a real problem encountered during the experiment. Firstly, as many as 156 research sessions were not completed (abandoned or interrupted), of which several of them (reported by respondents) resulted from the suspension of the research due to a weak Internet connection, and some resulted from prematurely closing the browser after the research was completed, but still while uploading results to the cloud. When it comes to boredom, there were feedbacks that the test was interrupted during the intermediate calibration or at the third displayed advertisement, so it should be considered to shorten further remote tests in the future.
4. Data analysis procedure 
To analyze the online study, the first step was to download a data package containing recorded research sessions in the form of a .zip archive from the iMotions cloud and import it into a project within the iMotions Biometric Platform 9.0 software on a local computer.
Then, based on the recordings, it was necessary to segment the respondents into technology enthusiasts and non-enthusiasts and create specific stimulus analyses in the software to export the data in tabular form. The work began with exporting the survey results and creating summaries and analyses of those results. Next, based on the survey results, areas of interest (AOIs) were marked for the objects that were identified as most attention-grabbing. After applying AOIs, heatmaps and numerical summaries were generated for the eye-tracking data. The next step involved generating emotion data by applying the standard threshold of 50 for the AFFECTIVA algorithm and exporting R-processed notebooks to obtain an averaged result for the experienced emotions during the display of each advertisement. Subsequently, for clarity and improved efficiency in working with all the data sets, they were imported into a shared spreadsheet and divided into appropriate sheets for visualization in the form of tables.
In summary, the work was done for all the stimuli: 5 surveys and 4 advertisements, resulting in a final dataset containing data for 90 respondents. The analysis for AFFECTIVA included 7 emotions: joy, anger, fear, disgust, contempt, sadness, surprise, and none of the above (survey option). For eye-tracking, the following areas of interest were selected based on survey results: Dell advertisement - car, Microsoft advertisement - boy, HP advertisement - vampires (woman and man), Google advertisement - pink hair woman. All calculations and summarizing work were done in R language using RStudio, then exported to Microsoft Excel to make some tables. When it comes to the eye tracking measurements – tables and heatmaps - they were visualised and exported in graphics form directly from iMotions and then in several cases combined in Adobe Photoshop to make it more clear for this paper.
5. Project results
A segmentation of all respondents into three groups was conducted based on the collected survey data. These groups include technology enthusiasts (61 individuals, 67.78%) and non-enthusiasts (29 individuals, 32.22%), making a total of 90 respondents. Among all respondents, women accounted for a significant 60%, while men constituted 40% of the sample.


Table 1. Segmentation.
  

5.1. Dell advertisement
According to the results regarding opinions on the advertisement and the brand, we learn that among all respondents, a remarkable 90% were familiar with the Dell brand. Among enthusiasts, this number was 91.80%, while among non-enthusiasts, it was 86.21%. A few individuals did not remember the specific brand featured in the advertisement, and less than 7% admitted to being unfamiliar with the brand across all groups. These results indicate a high brand recognition for Dell. As much as 90% of all respondents stated that they liked the advertisement (91.8% among enthusiasts and 86.21% among non-enthusiasts). However, 10% of all respondents (8.2% of enthusiasts and 13.79% of non-enthusiasts) did not find the advertisement appealing. Interestingly, this did not always translate into positive experiences with the brand's products. A significant percentage, 43.33% of all respondents (37.7% of enthusiasts and 55.17% of non-enthusiasts), had no prior experiences with the Dell brand. Among enthusiasts, positive experiences dominated at 42.62%, and none reported negative experiences with Dell. Despite the advertisement being well-liked by the participants, it did not effectively incentivize a majority from either group to purchase Dell's products or services. This was stated by 71.11% of all respondents, 68.85% of enthusiasts, and 75.86% of non-enthusiasts.
Moving on to comparing the emotions declared by the respondents with the emotions measured by the Affectiva algorithm, the most frequent response regarding the main emotion experienced, indicated by all respondent groups, was joy - 52.22% of all participants, 50.82% of enthusiasts, and 55.17% of non-enthusiasts. In this case, it aligns with the highest percentage of this emotion detected by the Affectiva algorithm throughout the entire duration of the advertisement - 17.45% of all participants, 11.59% of enthusiasts, and 36.98% of non-enthusiasts. It is worth noting that the survey allowed for the option "none of the above," which could have influenced the results.
In the subsequent optional question, which aimed to identify another emotion, all respondent groups chose surprise - 46.15% of all participants, 46.51% of enthusiasts, and 45.45% of non-enthusiasts. However, the Affectiva measurements indicated contempt - 5.81% of all participants, 4.68% of enthusiasts, and 6.75% of non-enthusiasts. It should be added that all other emotions had relatively low percentages compared to the dominant joy indicated by Affectiva.
Regarding the last question about an emotion that was certainly not felt while watching the advertisement, there appears to be agreement between the declarative data and the biometric data, as all the dominant emotions in the survey, namely anger, fear, and disgust, had very low scores in the biometric results across all respondent groups.


Table 2. AFFECTIVA emotions (Dell ad).
  



The survey results regarding the area of interest indicated as the most frequently observed by respondents in all groups - the car - are as follows: 64% of all participants, 62.65% of enthusiasts, and 76.19% of non-enthusiasts. These data align with the eye-tracking study for this object, which indicates that a significant number of participants indeed looked at the car: 91.1% of all participants, 93.4% of enthusiasts, and 86.2% of non-enthusiasts.
Furthermore, the total percentage of time spent observing the car in relation to the total gaze duration recorded for the advertisement (referred to as total time spent/dwell time) was 30.9% for all participants, 32% for enthusiasts, and 28.2% for non-enthusiasts.


  

Figure 3. Heatmap – car AOI (Dell ad).


Table 3. Eye-tracking results (Dell ad).
  

5.2. Microsoft advertisement
From the results regarding opinions about the advertisement and the brand, we learn that an impressive 97.78% of all respondents were familiar with the Microsoft brand. Among enthusiasts, this number was 98.36%, while among non-enthusiasts, it was 96.55%. All participants remembered the brand featured in the advertisement, and only a small percentage of respondents, 2.22% of all surveyed individuals, 3.45% of non-enthusiasts, and 1.64% of enthusiasts, claimed to be unfamiliar with the brand. These results demonstrate a very high brand recognition for Microsoft.
67.78% of all respondents stated that they liked the advertisement (73.77% of enthusiasts and 55.17% of non-enthusiasts), while 32.22% of all respondents, 26.23% of enthusiasts, and 44.83% of non-enthusiasts did not find the advertisement appealing. However, positive experiences with Microsoft's products were reported by 37.78% of all respondents, 34.43% of enthusiasts, and 44.83% of non-enthusiasts. Interestingly, among enthusiasts, moderate experiences dominated at 39.34%, and similar percentages were observed in other groups, close to 40%. Negative experiences with the brand were expressed by 13.11% of enthusiasts, 10.34% of non-enthusiasts, and 12.22% of all participants.
Despite the advertisement being well-liked by the majority of participants, it would not effectively persuade the majority of any group to purchase the company's products or services. This was declared by 54.44% of all participants, 55.74% of enthusiasts, and 51.72% of non-enthusiasts.
When it comes to the emotions declared by respondents compared to the emotions measured by the Affectiva algorithm, the most common response to the question about the primarily experienced emotion chosen by almost all groups of participants was "none of the above" - 28.89% of all respondents, 31.15% of enthusiasts, and 24.14% of non-enthusiasts (in the latter group, "joy" appeared more frequently). The second dominant emotion was surprise; however, this was not reflected in the data from the Affectiva module, where indications of this emotion were marginal. Joy as the primary and second experienced emotion mainly applied to non-enthusiasts, and this was reflected in the results of Affectiva. During the study, the participants certainly did not feel fear, as the results from the declarative and biometric study align in this area across all groups of respondents. However, there are discrepancies in the case of the other emotions depending on the respondent groups.


Table 4. AFFECTIVA emotions (MS ad).
  



The results of the survey regarding the area of interest indicated as the most frequently observed by respondents in all groups except enthusiasts (where Apple iPad prevailed) - a boy - are as follows: 32.5% of all respondents, 25.86% of enthusiasts, and 50% of non-enthusiasts indicated this area. The indications are lower than those from the eye-tracking study for this object, which show that 100% of respondents from all groups looked at this AOI.
Furthermore, the total percentage of time spent observing the boy relative to the gazes recorded for the advertisement (known as total time spent/dwell time) was 51.6% for all respondents, 51.9% for enthusiasts, and 51.1% for non-enthusiasts.


  

Figure 4. Heatmap – boy AOI (MS ad).


Table 5. Eye-tracking results – boy AOI (MS ad).
  

5.3. HP advertisement
The results regarding opinions about the advertisement itself and the brand indicate that in the group of all respondents, as many as 84.44% were familiar with the HP brand, while among enthusiasts, it was 81.97%, and among non-enthusiasts, 89.66%. A few individuals did not remember the brand featured in the advertisement, and the lack of familiarity with the brand itself was admitted by only 7.78% of all survey participants, 3.45% of non-enthusiasts, and 9.84% of enthusiasts. The results demonstrate a very high brand recognition for HP.
Regarding the liking of the advertisement, 63.33% of all respondents stated that they liked it (60.66% among enthusiasts and 68.97% among non-enthusiasts). In the same groups, the advertisement was not liked by 36.67% of all respondents, 39.34% of enthusiasts, and 31.03% of non-enthusiasts. Additionally, 32.22% of all participants reported positive experiences with the brand's products, 34.43% among enthusiasts, and 27.59% among non-enthusiasts. It is worth noting that moderate experiences dominated among all respondents and enthusiasts, with rates of 33.33% and 36.07%, respectively, while among non-enthusiasts, this indicator was 27.59%. Negative experiences with the brand were expressed by 9.84% of enthusiasts, 13.79% of non-enthusiasts, and 11.11% of all respondents.
Interestingly, despite the majority of participants liking the advertisement, it would not effectively persuade most individuals from any of the groups to purchase the company's service or product. This was stated by as many as 73.33% of all respondents, 72.13% of enthusiasts, and 75.86% of non-enthusiasts.
The comparison between the emotions declared by the respondents and those measured by Affectiva reveals that the predominant emotion experienced by all groups of respondents was joy – 36.67% of all participants, 32.79% of enthusiasts, and 44.83% of non-enthusiasts. In this case, it aligns with the highest percentage indications of this emotion during the entire duration of the advertisement according to the Affectiva algorithm – 17.43% of all participants, 13.72% of enthusiasts, and 22.53% of non-enthusiasts.
In the subsequent optional question, which aimed to identify another emotion, all groups of respondents selected surprise – 37.70% of all participants, 37.21% of enthusiasts, and 38.89% of non-enthusiasts, while the Affectiva data indicated disgust – 11.37% of all participants, 10.46% of enthusiasts, and 16.87% of non-enthusiasts. However, it is worth mentioning that surprise was also clearly identified as an emotion by Affectiva. Moving on to the last question about an emotion that is certainly not felt, there is a discrepancy between the declarative and biometric data because joy, which was most frequently chosen in the survey, turned out to be the most strongly experienced emotion according to the biometric measurements. As for anger, fear, and sadness, the indications can be considered similar.


Table 6. AFFECTIVA emotions (HP ad).
  



The results of the survey regarding the area of interest indicated as most frequently observed by the respondents in all groups – vampires – are as follows: 45.45% among all participants, 47.62% of enthusiasts, and 40% of non-enthusiasts. For the purpose of analyzing the areas of interest, AOIs (Areas of Interest) were drawn separately for each of the two vampires. The indications recorded in the survey aligned with the eye-tracking study for these objects, meaning that in the case of the male vampire, a whopping 97.8% of all participants, 98.4% of enthusiasts, and 96.4% of non-enthusiasts actually looked at this object. The indications for the female vampire were similar, with respective percentages of 98.9%, 98.4%, and 100%. Additionally, the total percentage of time spent observing the male vampire relative to the total gaze time recorded for the advertisement (known as total time spent/dwell time) was 42.6% for all participants, 43.7% for enthusiasts, and 40.3% for non-enthusiasts. The same indications for the female vampire were 54.8%, 55.8%, and 52.8%, respectively.


  

Figure 5. Heatmap – vampires AOI (HP ad).


Table 7. Eye-tracking results – vampires AOI (HP ad).
  

5.4. Google advertisement
According to the results concerning opinions about the advertisement itself and the brand, we learn that in the group of all respondents, a staggering 95.56% were familiar with the Google brand, while among enthusiasts, it was 95.08%, and among non-enthusiasts, it was 96.55%. Only a few individuals didn't remember the brand featured in the advertisement, and one enthusiast admitted unfamiliarity with the brand, accounting for 1.64% of that group and 1.11% of all participants. The results indicate a very high brand recognition for Google. 86.67% of all respondents stated that they liked the advertisement (88.52% among enthusiasts and 82.76% among non-enthusiasts), while 13.33% of all participants, 11.48% of enthusiasts, and 17.24% of non-enthusiasts did not like the advertisement. At the same time, a whopping 75.56% of all respondents had positive experiences with Google products, with 77.05% among enthusiasts and 72.41% among non-enthusiasts. As for moderate experiences, the respective percentages were only 16.67% for all participants, 14.75% for enthusiasts, and 20.69% for non-enthusiasts. Negative experiences with the brand were reported by 3.28% of enthusiasts, 3.45% of non-enthusiasts, and 3.33% of all participants. It is noteworthy that this was the only advertisement that would encourage a significant majority of each group to purchase the company's service or product (as declared by 61.11% of all participants, 63.93% of enthusiasts, and 55.17% of non-enthusiasts).
Turning to the comparison of emotions declared by respondents and those measured by the Affectiva algorithm: the most common response regarding the primarily experienced emotion indicated by all respondent groups was joy – 67.78% of all participants, 70.49% of enthusiasts, and 62.07% of non-enthusiasts. In this case, it aligns with the highest percentage of this emotion's presence measured by the Affectiva algorithm throughout the entire duration of the advertisement – 16.42% of all respondents, 10.06% of enthusiasts, and 41.87% of non-enthusiasts. 
In the next optional question regarding another felt emotion, all respondent groups except for non-enthusiasts (with a slight deviation in favor of joy) chose surprise – 50.85% of all participants, 56.10% of enthusiasts, and 38.89% of non-enthusiasts. However, Affectiva identified surprise as the second emotion only for non-enthusiasts (6.15%). It is worth mentioning that anger was clearly noted as the second emotion in sequence by Affectiva. Responses to the question about an emotion that certainly did not accompany the participants while watching the advertisement show agreement regarding fear and sadness between declarative and biometric data. However, the same cannot be said for anger, which is identified as the second felt emotion by Affectiva while respondents indicated it as certainly not experienced.


Table 8. AFFECTIVA emotions (Google ad).
  



The results of the survey regarding the area of interest indicated as the most frequently observed by respondents in all groups - a woman with pink hair - are as follows: 65.88% of all participants, 58.06% of enthusiasts, and 86.96% of non-enthusiasts indicated this area of interest. These findings are lower than those from the eye-tracking study, which show that 95.6% of all participants, 96.7% of enthusiasts, and 93.1% of non-enthusiasts looked at this object. 
Furthermore, the total percentage of time spent observing the woman in relation to the gazes recorded for the advertisement (known as total time spent/dwell time) was 55% for all participants, 57.2% for enthusiasts, and 50.3% for non-enthusiasts.


  

Figure 6. Heatmap – pink hair woman AOI (Google ad).


Table 9. Eye-tracking results – pink hair woman AOI (Google ad).
  

6. Discussion
The question posed in the title arises - Is this a viable alternative to laboratory-based studies? The answer seems ambiguous, and perhaps the most honest response would be - it depends.
Certainly, in laboratory conditions, we are able to better ensure the standardization of the research procedure - consistent equipment, the choice of eye tracking measurement method using a stationary eyetracker, a high-quality camera for measuring emotions, as well as uniform and excellent lighting conditions. Furthermore, a standardized research setup allows us to ensure that the respondent's position in front of the computer is correct.
The question that needs to be asked, however, is whether a costly and time-consuming laboratory procedure is necessary for every application and type of study? Based on the results, it seems that remote research is viable, especially for marketing studies, as technology, particularly web-based eye tracking, is advancing rapidly. Additionally, considering the results related to emotion research (sampling quality), it can be concluded that the webcams available to respondents are of sufficient quality, and the lack of standardization for the project is not of great significance. Moreover, the algorithm checking lightening conditions and position of respondent works fine and eliminates people non-meeting expected standards. Another factor is the organization and arrangement of visits to the laboratory, which is time-consuming, and instances of respondents not showing up or breaking appointments are common. Therefore, the issue of drop rate in remote studies is not as significant since more people can participate in the study at the same time, and the researcher's time commitment is limited to recruiting participants and sending out the link.
It seems, therefore, that if the project specifically focuses on studying emotions using the AFFECTIVA algorithm and collects survey data, conducting the study remotely is a very good idea. In such circumstances, there are no visible negative consequences of choosing this approach, and the advantages related to time savings are evident. Since incorporating an eye tracking module based on a camera does not require any additional actions from the researcher, except for selecting the appropriate option during the study design phase and adding additional calibration slides, this element can be considered an interesting addition. However, it should be taken into account that adding this functionality will lengthen the study itself due to the three calibration stages.
Therefore, one can argue that yes - remote biometric research makes sense and is a viable alternative to laboratory-based studies, as long as we are willing to accept lower eye tracking effectiveness based on a webcam and high sampling quality is not crucial to us. We can consider it as an additional option or be willing to forego it. 
7. Conclusions
Firstly, the emotions declared in the survey, both experienced and not experienced, did not always align with those indicated by the measurements taken by the Affectiva algorithm. Among the reasons for this can be the inability to label emotions accurately or the reluctance to do so, influenced by personal preferences or biases towards certain brands (as seen, for example, in the Microsoft advertisement, which compared their product to a competitor's, leading to diverse results due to the ongoing rivalry with Apple fans). Another factor to consider is the possibility of misclassification of emotions by the algorithm.
However, as explained earlier, Affectiva is based on a neural network that analyzes various facial muscles, and the overall measurement quality for the entire study was very high, reaching 98% (24Hz), which is an outstanding result considering that participants took part remotely using different equipment.
Secondly, in response to the question of how we look at advertisements, it should be noted that the eye-tracking component performed surprisingly well using webcams. The measurement quality was assessed at an impressive 70% with 24Hz, and the results comparing biometric data with declarative surveys showed significant similarities. It is evident that the selected advertising materials were well-designed by their creators. The areas of interest identified for further analysis not only aligned with the most frequently reported by respondents in the survey but also represented crucial elements of the ads themselves, their presentation concept, and the promotion of the respective brand, service, or products.
It is important to note that there should be no illusions that the results obtained with a stationary eye-tracker would be significantly more reliable (the aforementioned DVA metric and heatmaps speak in favor of professional equipment). However, the results and their credibility can be further examined, for example, by marking additional areas of interest (AOI) for less dominant objects based on respondents' indications in the surveys, and subject them to further analysis. Despite the limitations described in the theoretical part, the performance of the algorithm itself is promising.
In comparison to stationary studies, no difference was observed in the method and quality of the survey completion mechanism. The system functioned efficiently and was intuitive for respondents. The exact same situation applies to the display of instructions and the stimuli themselves - images and videos. There were occasional instances related to poor internet connection quality among respondents, but these were random occurrences and difficult to attribute to the specific module responsible for conducting remote studies.
First thing that should be omitted in attempting another remote study is the assessment of baseline emotions based on the Karolinska Directed Emotional Faces (KDEF) image set. The main purpose was to verify whether the emotions inherently depicted in the photos were reflected in the respondents' reactions recorded by the Affectiva algorithm. However, it turned out to be unnecessary since the reference database and effectiveness of the Affectiva algorithm have been extensively studied and proven (Girard et al., 2013; Littlewort et al., 2011; McDuff et al., 2015; El Kaliouby & Robinson, 2005). Conducting a baseline emotion study is not required when using the facial emotion analysis module's functionality. Furthermore, respondents did not always understand the instruction to mimic the facial expression based on the images, and the procedure itself unnecessarily prolonged the study.
Another aspect that should be improved is to shorten the entire experiment to avoid unsuccessful or incomplete attempts and approaches to the study. In addition to removing the baseline emotion study, as mentioned earlier, it would be advisable to consider shortening the surveys or even omitting the display of one of the advertisements.
Third area that could be improved to enhance the user-friendliness of studies utilizing web-cam based eye tracking is the calibration process. The current calibration procedure, which involves three stages - initial, intermediate, and final, not only lengthens the overall study duration but also contributes to respondent impatience. 
8. References 
Aroganam, G., Manivannan, N., & Harrison, D. (2019). Review on Wearable Technology Sensors Used in Consumer Sport Applications. Sensors (Basel, Switzerland), 19(9), 1983.
El Kaliouby, R., & Robinson, P. (2005). Real-time inference of mental states from facial expressions and head gestures for intelligent tutoring systems. Applied Artificial Intelligence, 19(3-4), 205-238.
Girard, J. M., Cohn, J. F., Mahoor, M. H., Mavadati, S. M., & Rosenwald, D. P. (2013). Social risk and depression: Evidence from manual and automatic facial expression analysis. IEEE Transactions on Affective Computing, 4(3), 290-301.
Healey, J. A., & Picard, R. W. (2005). Detecting stress during real-world driving tasks using physiological sensors. IEEE Transactions on Intelligent Transportation Systems, 6(2), 156-166.
Klaib, A. F., Alsrehin, N. O., Melhem, W. Y., Bashtawi, H. O., & Magableh, A. A. (2021). Eye tracking algorithms, techniques, tools, and applications with an emphasis on machine learning and Internet of Things technologies. Expert Systems With Applications, 166, 114037.
Krafka, K., Khosla, A., Kellnhofer, P., Kannan, H., Bhandarkar, S., Matusik, W., & Torralba, A. (2020). Eye tracking for everyone. IEEE Conference on Computer Vision and Pattern Recognition (CVPR).
Littlewort, G., Whitehill, J., Wu, T., Fasel, I., Frank, M., Movellan, J., & Bartlett, M. S. (2011). The computer expression recognition toolbox (CERT). In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops (pp. 213-220).
Maurya, L., Zwiggelaar, R., Chawla, D., & Mahapatra, P. (2023). Non-contact respiratory rate monitoring using thermal and visible imaging: a pilot study on neonates. Journal of clinical monitoring and computing, 37(3), 815–828.
McDuff, D., & el Kaliouby, R. (2013). Crowdsourcing facial responses to online videos for continuous emotion annotation. IEEE Transactions on Affective Computing, 4, 70-80.
McDuff, D., El Kaliouby, R., Kassam, K. S., & Picard, R. W. (2015). Affectiva-MIT facial expression dataset (AM-FED): Naturalistic and spontaneous facial expressions collected "in-the-wild". In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops (pp. 1-10).
Recasens, A., Khosla, A., Vondrick, C., & Torralba, A. (2015). Where are they looking? In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 2947-2954).
Sugano, Y., Matsushita, Y., & Sato, Y. (2014). Learning-by-synthesis for appearance-based 3D gaze estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 1821-1828).
Wood, E., Baltrušaitis, T., Morency, L. P., Robinson, P., & Bulling, A. (2016). Learning an appearance-based gaze estimator from one million synthesized images. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 1880-1889).